{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Model Training and Comparison\n",
    "\n",
    "## Step 1: Import Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "```\n",
    "\n",
    "## Step 2: Load and Prepare the Data\n",
    "\n",
    "```python\n",
    "# Assume preprocessed and resampled data are already available\n",
    "X_train_scaled = X_train_scaled  # scaled training features\n",
    "X_test_scaled = X_test_scaled    # scaled test features\n",
    "y_train = y_train\n",
    "y_test = y_test\n",
    "```\n",
    "\n",
    "## Step 3: Define and Train Models\n",
    "\n",
    "### Logistic Regression\n",
    "```python\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "### Random Forest\n",
    "```python\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "### XGBoost\n",
    "```python\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "### Neural Network with Keras\n",
    "```python\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train, validation_split=0.2, epochs=50, batch_size=64, callbacks=[es], verbose=0)\n",
    "```\n",
    "\n",
    "## Step 4: Evaluate Models\n",
    "\n",
    "```python\n",
    "def evaluate_model(model, X_test, y_test, is_keras=False):\n",
    "    if is_keras:\n",
    "        y_pred_proba = model.predict(X_test).ravel()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    return y_pred_proba, classification_report(y_test, y_pred, output_dict=True), roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Logistic Regression\n",
    "_, cr, auc = evaluate_model(log_model, X_test_scaled, y_test)\n",
    "results['Logistic Regression'] = {'Classification Report': cr, 'ROC_AUC': auc}\n",
    "\n",
    "# Random Forest\n",
    "_, cr, auc = evaluate_model(rf_model, X_test_scaled, y_test)\n",
    "results['Random Forest'] = {'Classification Report': cr, 'ROC_AUC': auc}\n",
    "\n",
    "# XGBoost\n",
    "_, cr, auc = evaluate_model(xgb_model, X_test_scaled, y_test)\n",
    "results['XGBoost'] = {'Classification Report': cr, 'ROC_AUC': auc}\n",
    "\n",
    "# Neural Network\n",
    "_, cr, auc = evaluate_model(nn_model, X_test_scaled, y_test, is_keras=True)\n",
    "results['Neural Network (Keras)'] = {'Classification Report': cr, 'ROC_AUC': auc}\n",
    "```\n",
    "\n",
    "## Step 5: Final Comparison\n",
    "\n",
    "```python\n",
    "# Compare all models\n",
    "performance_data = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    precision_1 = metrics['Classification Report']['1.0']['precision']\n",
    "    recall_1 = metrics['Classification Report']['1.0']['recall']\n",
    "    f1_score_1 = metrics['Classification Report']['1.0']['f1-score']\n",
    "\n",
    "    performance_data.append({\n",
    "        'Model': model_name,\n",
    "        'ROC_AUC': metrics['ROC_AUC'],\n",
    "        'Precision_1': precision_1,\n",
    "        'Recall_1': recall_1,\n",
    "        'F1-Score_1': f1_score_1\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values(by='ROC_AUC', ascending=False)\n",
    "print(performance_df)\n",
    "```\n",
    "\n",
    "## Step 6: Plot ROC Curves\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name in results.keys():\n",
    "    if model_name == 'Neural Network (Keras)':\n",
    "        y_pred_proba = nn_model.predict(X_test_scaled).ravel()\n",
    "    else:\n",
    "        y_pred_proba = eval(f\"{model_name.lower().replace(' ', '_')}_model\").predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## Step 7: Conclusion\n",
    "\n",
    "```python\n",
    "print(\"\\n--- Final Conclusion ---\")\n",
    "best_model = performance_df.iloc[0]['Model']\n",
    "print(f\"Based on ROC AUC and F1-Score for the minority class, the best performing model is likely: {best_model}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b206e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"--- Step 1: Loading and Preprocessing Data ---\")\n",
    "dataset = pd.read_csv('Extracted-dataset.csv')\n",
    "print(\"Original Dataset Shape:\", dataset.shape)\n",
    "dataset.head()\n",
    "# --- STEP 1: Encode binary target/label columns ---\n",
    "dataset['RainToday'] = dataset['RainToday'].map({'No': 0, 'Yes': 1})\n",
    "dataset['RainTomorrow'] = dataset['RainTomorrow'].map({'No': 0, 'Yes': 1})\n",
    "dataset.head()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Check for fully missing columns\n",
    "fully_missing_cols = dataset.columns[dataset.isnull().all()]\n",
    "print(\"Columns with all missing values:\", fully_missing_cols.tolist())\n",
    "\n",
    "# --- STEP 2: Handle Missing Values ---\n",
    "print(\"--- Step 2: Handling Missing Values ---\")\n",
    "\n",
    "# Step 2.1: Impute object/categorical columns with mode\n",
    "categorical_cols = dataset.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    mode_value = dataset[col].mode()[0]\n",
    "    print(f\"Imputing missing values in column '{col}' with mode value: '{mode_value}'\")\n",
    "    dataset[col] = dataset[col].fillna\n",
    "\n",
    "\n",
    "# Step 2.2: Label Encode categorical columns\n",
    "print(\"Label encoding categorical columns...\")\n",
    "lencoders = {}\n",
    "for col in categorical_cols:\n",
    "    lencoders[col] = LabelEncoder()\n",
    "    dataset[col] = lencoders[col].fit_transform(dataset[col])\n",
    "\n",
    "# Step 2.3: Impute numerical columns using Iterative Imputer (MICE)\n",
    "print(\"Running MICE imputation on numerical features...\")\n",
    "mice_imputer = IterativeImputer(random_state=42)\n",
    "dataset_imputed_array = mice_imputer.fit_transform(dataset)\n",
    "\n",
    "# Ensure shape matches before converting to DataFrame\n",
    "if dataset_imputed_array.shape[1] != dataset.shape[1]:\n",
    "    print(\"Warning: Imputed array shape does not match original dataset. Fixing...\")\n",
    "    dataset = dataset.iloc[:, :dataset_imputed_array.shape[1]]\n",
    "\n",
    "dataset_imputed = pd.DataFrame(dataset_imputed_array, columns=dataset.columns)\n",
    "\n",
    "# Final check\n",
    "print(\"Shape after imputation:\", dataset_imputed.shape)\n",
    "dataset_imputed.head(6)\n",
    "# Step 1: Keep only numeric columns (needed for Pearson correlation)\n",
    "numeric_df = dataset_imputed.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Step 2: Get correlation of each numeric column with 'RainTomorrow'\n",
    "correlation_with_target = numeric_df.corr()['RainTomorrow'].sort_values(ascending=False)\n",
    "\n",
    "# Step 3: Print correlation values\n",
    "print(\"\\n--- Pearson Correlation with 'RainTomorrow' ---\")\n",
    "print(correlation_with_target)\n",
    "sns.barplot(\n",
    "    x=correlation_with_target.drop('RainTomorrow').values,\n",
    "    y=correlation_with_target.drop('RainTomorrow').index                #  .index gives us the row labels — which, in this case, are the names of the features\n",
    ")\n",
    "# Make a copy to avoid changing the original data unintentionally\n",
    "data = dataset_imputed.copy()\n",
    "\n",
    "# Remove 'RISK_MM' if it exists\n",
    "if 'RISK_MM' in data.columns:\n",
    "    data.drop('RISK_MM', axis=1, inplace=True)\n",
    "    print(\"'RISK_MM' feature removed to prevent data leakage.\")\n",
    "else:\n",
    "    print(\"'RISK_MM' already removed or not present.\")\n",
    "\n",
    "# Display the shape and the first few rows to confirm\n",
    "print(\"\\nUpdated dataset shape:\", data.shape)\n",
    "print(\"\\nFirst 5 rows after removing 'RISK_MM':\")\n",
    "display(data.head())\n",
    "# X = all features except 'RainTomorrow'\n",
    "cleaned = data.copy()\n",
    "# y = target variable\n",
    "X = cleaned.drop('RainTomorrow', axis=1)\n",
    "y = cleaned['RainTomorrow']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Check class balance\n",
    "print(\"Original Class Distribution:\\n\", y.value_counts(normalize=True))\n",
    "print(\"Training Set Class Distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "print(\"Testing Set Class Distribution:\\n\", y_test.value_counts(normalize=True))\n",
    "\n",
    "# Plot class distribution in original training data\n",
    "\n",
    "# Make sure y_train is a DataFrame with column name\n",
    "y_train_df = pd.DataFrame(y_train, columns=['RainTomorrow'])\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=y_train_df, x='RainTomorrow', hue='RainTomorrow', palette='pastel', legend=False)\n",
    "plt.title(\"Class Distribution Before Oversampling\")\n",
    "plt.xlabel(\"RainTomorrow (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Assuming X_train and y_train are already defined\n",
    "print(\"\\n--- Step 3: Oversampling and Handling Outliers on Training Data ---\")\n",
    "df_train_combined = pd.concat([X_train, y_train], axis=1)\n",
    "minority_class = df_train_combined[df_train_combined['RainTomorrow'] == 1]\n",
    "majority_class = df_train_combined[df_train_combined['RainTomorrow'] == 0]\n",
    "\n",
    "if not minority_class.empty:\n",
    "    minority_oversampled = resample(minority_class,\n",
    "                                    replace=True,\n",
    "                                    n_samples=len(majority_class),\n",
    "                                    random_state=42)\n",
    "    df_train_resampled = pd.concat([majority_class, minority_oversampled])\n",
    "else:\n",
    "    print(\"⚠️ Minority class is empty. Oversampling not performed.\")\n",
    "    df_train_resampled = df_train_combined.copy()\n",
    "\n",
    "print(\"Class distribution after oversampling:\\n\", df_train_resampled['RainTomorrow'].value_counts())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(data=df_train_resampled, x='RainTomorrow', hue='RainTomorrow', palette='pastel', legend=False)\n",
    "\n",
    "plt.title(\"Class Distribution After Oversampling\", fontsize=14)\n",
    "plt.xlabel(\"Rain Tomorrow (0 = No, 1 = Yes)\", fontsize=12)\n",
    "plt.ylabel(\"Count\", fontsize=12)\n",
    "plt.xticks([0, 1], ['No', 'Yes'])\n",
    "\n",
    "plt.show()\n",
    "# Now, remove outliers from the BALANCED training data\n",
    "Q1 = df_train_resampled.quantile(0.25)\n",
    "Q3 = df_train_resampled.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "cleaned_resampled = df_train_resampled[~((df_train_resampled < (Q1 - 1.5 * IQR)) | (df_train_resampled > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "X_train_final = cleaned_resampled.drop('RainTomorrow', axis=1)\n",
    "y_train_final = cleaned_resampled['RainTomorrow']\n",
    "\n",
    "print(\"Final shape after outlier removal:\", cleaned_resampled.shape)\n",
    "print(\"Final class distribution:\\n\", y_train_final.value_counts(normalize=True))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "\n",
    "print(\"\\n--- Step 4: Training and Comparing Models ---\")\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'),\n",
    "    'XGBoost': XGBClassifier(n_estimators=100, random_state=42,  eval_metric='logloss')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    model.fit(X_train_final, y_train_final)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_pred_proba),\n",
    "        'Classification Report': classification_report(y_test, y_pred, output_dict=True)\n",
    "    }\n",
    "\n",
    "    print(f\"--- {model_name} Evaluation on Test Data ---\")\n",
    "    print(f\"ROC AUC Score: {results[model_name]['ROC_AUC']:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Rain', 'Rain'])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    disp.ax_.set_title(f\"Confusion Matrix for {model_name}\")\n",
    "    plt.show()\n",
    "    print(\"\\n--- Step 5: Final Model Comparison Summary ---\")\n",
    "\n",
    "# Step 1: Collect rows in a list\n",
    "performance_data = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    precision_1 = metrics['Classification Report']['1.0']['precision']\n",
    "    recall_1 = metrics['Classification Report']['1.0']['recall']\n",
    "    f1_score_1 = metrics['Classification Report']['1.0']['f1-score']\n",
    "    \n",
    "    performance_data.append({\n",
    "        'Model': model_name,\n",
    "        'ROC_AUC': metrics['ROC_AUC'],\n",
    "        'Precision_1': precision_1,\n",
    "        'Recall_1': recall_1,\n",
    "        'F1-Score_1': f1_score_1\n",
    "    })\n",
    "\n",
    "# Step 2: Convert to DataFrame\n",
    "performance_df = pd.DataFrame(performance_data)\n",
    "performance_df = performance_df.sort_values(by='ROC_AUC', ascending=False)\n",
    "print(performance_df)\n",
    "\n",
    "# Step 3: Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for model_name, model in models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Conclusion\n",
    "print(\"\\n--- Final Conclusion ---\")\n",
    "best_model = performance_df.iloc[0]['Model']\n",
    "print(f\"Based on ROC AUC and F1-Score for the minority class, the best performing model is likely: {best_model}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
